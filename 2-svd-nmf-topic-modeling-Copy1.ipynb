{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Topic Modeling with NMF and SVD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The problem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Topic modeling is a fun way to start our study of NLP. We will use two popular **matrix decomposition techniques**. \n",
    "\n",
    "We start with a **term-document matrix**:\n",
    "\n",
    "<img src=\"images/document_term.png\" alt=\"term-document matrix\" style=\"width: 80%\"/>\n",
    "\n",
    "source: [Introduction to Information Retrieval](http://player.slideplayer.com/15/4528582/#)\n",
    "\n",
    "We can decompose this into one tall thin matrix times one wide short matrix (possibly with a diagonal matrix in between).\n",
    "\n",
    "Notice that this representation does not take into account word order or sentence structure.  It's an example of a **bag of words** approach."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Latent Semantic Analysis (LSA) uses Singular Value Decomposition (SVD)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Motivation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider the most extreme case - reconstructing the matrix using an outer product of two vectors. Clearly, in most cases we won't be able to reconstruct the matrix exactly. But if we had one vector with the relative frequency of each vocabulary word out of the total word count, and one with the average number of words per document, then that outer product would be as close as we can get.\n",
    "\n",
    "Now consider increasing that matrices to two columns and two rows. The optimal decomposition would now be to cluster the documents into two groups, each of which has as different a distribution of words as possible to each other, but as similar as possible amongst the documents in the cluster. We will call those two groups \"topics\". And we would cluster the words into two groups, based on those which most frequently appear in each of the topics. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting started"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll take a dataset of documents in several different categories, and find topics (consisting of groups of words) for them.  Knowing the actual categories helps us evaluate if the topics we find make sense.\n",
    "\n",
    "We will try this with two different matrix factorizations: **Singular Value Decomposition (SVD)** and **Non-negative Matrix Factorization (NMF)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn import decomposition\n",
    "from scipy import linalg\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "np.set_printoptions(suppress=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Additional Resources"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- [Data source](http://scikit-learn.org/stable/datasets/twenty_newsgroups.html): Newsgroups are discussion groups on Usenet, which was popular in the 80s and 90s before the web really took off.  This dataset includes 18,000 newsgroups posts with 20 topics.\n",
    "- [Chris Manning's book chapter](https://nlp.stanford.edu/IR-book/pdf/18lsi.pdf) on matrix factorization and LSI \n",
    "- Scikit learn [truncated SVD LSI details](http://scikit-learn.org/stable/modules/decomposition.html#lsa)\n",
    "\n",
    "### Other Tutorials\n",
    "- [Scikit-Learn: Out-of-core classification of text documents](http://scikit-learn.org/stable/auto_examples/applications/plot_out_of_core_classification.html): uses [Reuters-21578](https://archive.ics.uci.edu/ml/datasets/reuters-21578+text+categorization+collection) dataset (Reuters articles labeled with ~100 categories), HashingVectorizer\n",
    "- [Text Analysis with Topic Models for the Humanities and Social Sciences](https://de.dariah.eu/tatom/index.html): uses [British and French Literature dataset](https://de.dariah.eu/tatom/datasets.html) of Jane Austen, Charlotte Bronte, Victor Hugo, and more"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Look at our data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scikit Learn comes with a number of built-in datasets, as well as loading utilities to load several standard external datasets. This is a [great resource](http://scikit-learn.org/stable/datasets/), and the datasets include Boston housing prices, face images, patches of forest, diabetes, breast cancer, and more.  We will be using the newsgroups dataset.\n",
    "\n",
    "Newsgroups are discussion groups on Usenet, which was popular in the 80s and 90s before the web really took off.  This dataset includes 18,000 newsgroups posts with 20 topics.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "categories = ['alt.atheism', 'talk.religion.misc', 'comp.graphics', 'sci.space']\n",
    "remove = ('headers', 'footers', 'quotes')\n",
    "newsgroups_train = fetch_20newsgroups(subset='train', categories=categories, remove=remove)\n",
    "newsgroups_test = fetch_20newsgroups(subset='test', categories=categories, remove=remove)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['data', 'filenames', 'target_names', 'target', 'DESCR']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[n for n in newsgroups_train]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'.. _20newsgroups_dataset:\\n\\nThe 20 newsgroups text dataset\\n------------------------------\\n\\nThe 20 newsgroups dataset comprises around 18000 newsgroups posts on\\n20 topics split in two subsets: one for training (or development)\\nand the other one for testing (or for performance evaluation). The split\\nbetween the train and test set is based upon a messages posted before\\nand after a specific date.\\n\\nThis module contains two loaders. The first one,\\n:func:`sklearn.datasets.fetch_20newsgroups`,\\nreturns a list of the raw texts that can be fed to text feature\\nextractors such as :class:`sklearn.feature_extraction.text.CountVectorizer`\\nwith custom parameters so as to extract feature vectors.\\nThe second one, :func:`sklearn.datasets.fetch_20newsgroups_vectorized`,\\nreturns ready-to-use features, i.e., it is not necessary to use a feature\\nextractor.\\n\\n**Data Set Characteristics:**\\n\\n    =================   ==========\\n    Classes                     20\\n    Samples total            18846\\n    Dimensionality               1\\n    Features                  text\\n    =================   ==========\\n\\nUsage\\n~~~~~\\n\\nThe :func:`sklearn.datasets.fetch_20newsgroups` function is a data\\nfetching / caching functions that downloads the data archive from\\nthe original `20 newsgroups website`_, extracts the archive contents\\nin the ``~/scikit_learn_data/20news_home`` folder and calls the\\n:func:`sklearn.datasets.load_files` on either the training or\\ntesting set folder, or both of them::\\n\\n  >>> from sklearn.datasets import fetch_20newsgroups\\n  >>> newsgroups_train = fetch_20newsgroups(subset=\\'train\\')\\n\\n  >>> from pprint import pprint\\n  >>> pprint(list(newsgroups_train.target_names))\\n  [\\'alt.atheism\\',\\n   \\'comp.graphics\\',\\n   \\'comp.os.ms-windows.misc\\',\\n   \\'comp.sys.ibm.pc.hardware\\',\\n   \\'comp.sys.mac.hardware\\',\\n   \\'comp.windows.x\\',\\n   \\'misc.forsale\\',\\n   \\'rec.autos\\',\\n   \\'rec.motorcycles\\',\\n   \\'rec.sport.baseball\\',\\n   \\'rec.sport.hockey\\',\\n   \\'sci.crypt\\',\\n   \\'sci.electronics\\',\\n   \\'sci.med\\',\\n   \\'sci.space\\',\\n   \\'soc.religion.christian\\',\\n   \\'talk.politics.guns\\',\\n   \\'talk.politics.mideast\\',\\n   \\'talk.politics.misc\\',\\n   \\'talk.religion.misc\\']\\n\\nThe real data lies in the ``filenames`` and ``target`` attributes. The target\\nattribute is the integer index of the category::\\n\\n  >>> newsgroups_train.filenames.shape\\n  (11314,)\\n  >>> newsgroups_train.target.shape\\n  (11314,)\\n  >>> newsgroups_train.target[:10]\\n  array([ 7,  4,  4,  1, 14, 16, 13,  3,  2,  4])\\n\\nIt is possible to load only a sub-selection of the categories by passing the\\nlist of the categories to load to the\\n:func:`sklearn.datasets.fetch_20newsgroups` function::\\n\\n  >>> cats = [\\'alt.atheism\\', \\'sci.space\\']\\n  >>> newsgroups_train = fetch_20newsgroups(subset=\\'train\\', categories=cats)\\n\\n  >>> list(newsgroups_train.target_names)\\n  [\\'alt.atheism\\', \\'sci.space\\']\\n  >>> newsgroups_train.filenames.shape\\n  (1073,)\\n  >>> newsgroups_train.target.shape\\n  (1073,)\\n  >>> newsgroups_train.target[:10]\\n  array([0, 1, 1, 1, 0, 1, 1, 0, 0, 0])\\n\\nConverting text to vectors\\n~~~~~~~~~~~~~~~~~~~~~~~~~~\\n\\nIn order to feed predictive or clustering models with the text data,\\none first need to turn the text into vectors of numerical values suitable\\nfor statistical analysis. This can be achieved with the utilities of the\\n``sklearn.feature_extraction.text`` as demonstrated in the following\\nexample that extract `TF-IDF`_ vectors of unigram tokens\\nfrom a subset of 20news::\\n\\n  >>> from sklearn.feature_extraction.text import TfidfVectorizer\\n  >>> categories = [\\'alt.atheism\\', \\'talk.religion.misc\\',\\n  ...               \\'comp.graphics\\', \\'sci.space\\']\\n  >>> newsgroups_train = fetch_20newsgroups(subset=\\'train\\',\\n  ...                                       categories=categories)\\n  >>> vectorizer = TfidfVectorizer()\\n  >>> vectors = vectorizer.fit_transform(newsgroups_train.data)\\n  >>> vectors.shape\\n  (2034, 34118)\\n\\nThe extracted TF-IDF vectors are very sparse, with an average of 159 non-zero\\ncomponents by sample in a more than 30000-dimensional space\\n(less than .5% non-zero features)::\\n\\n  >>> vectors.nnz / float(vectors.shape[0])\\n  159.01327...\\n\\n:func:`sklearn.datasets.fetch_20newsgroups_vectorized` is a function which \\nreturns ready-to-use token counts features instead of file names.\\n\\n.. _`20 newsgroups website`: http://people.csail.mit.edu/jrennie/20Newsgroups/\\n.. _`TF-IDF`: https://en.wikipedia.org/wiki/Tf-idf\\n\\n\\nFiltering text for more realistic training\\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\\n\\nIt is easy for a classifier to overfit on particular things that appear in the\\n20 Newsgroups data, such as newsgroup headers. Many classifiers achieve very\\nhigh F-scores, but their results would not generalize to other documents that\\naren\\'t from this window of time.\\n\\nFor example, let\\'s look at the results of a multinomial Naive Bayes classifier,\\nwhich is fast to train and achieves a decent F-score::\\n\\n  >>> from sklearn.naive_bayes import MultinomialNB\\n  >>> from sklearn import metrics\\n  >>> newsgroups_test = fetch_20newsgroups(subset=\\'test\\',\\n  ...                                      categories=categories)\\n  >>> vectors_test = vectorizer.transform(newsgroups_test.data)\\n  >>> clf = MultinomialNB(alpha=.01)\\n  >>> clf.fit(vectors, newsgroups_train.target)\\n  MultinomialNB(alpha=0.01, class_prior=None, fit_prior=True)\\n\\n  >>> pred = clf.predict(vectors_test)\\n  >>> metrics.f1_score(newsgroups_test.target, pred, average=\\'macro\\')\\n  0.88213...\\n\\n(The example :ref:`sphx_glr_auto_examples_text_plot_document_classification_20newsgroups.py` shuffles\\nthe training and test data, instead of segmenting by time, and in that case\\nmultinomial Naive Bayes gets a much higher F-score of 0.88. Are you suspicious\\nyet of what\\'s going on inside this classifier?)\\n\\nLet\\'s take a look at what the most informative features are:\\n\\n  >>> import numpy as np\\n  >>> def show_top10(classifier, vectorizer, categories):\\n  ...     feature_names = np.asarray(vectorizer.get_feature_names())\\n  ...     for i, category in enumerate(categories):\\n  ...         top10 = np.argsort(classifier.coef_[i])[-10:]\\n  ...         print(\"%s: %s\" % (category, \" \".join(feature_names[top10])))\\n  ...\\n  >>> show_top10(clf, vectorizer, newsgroups_train.target_names)\\n  alt.atheism: edu it and in you that is of to the\\n  comp.graphics: edu in graphics it is for and of to the\\n  sci.space: edu it that is in and space to of the\\n  talk.religion.misc: not it you in is that and to of the\\n\\n\\nYou can now see many things that these features have overfit to:\\n\\n- Almost every group is distinguished by whether headers such as\\n  ``NNTP-Posting-Host:`` and ``Distribution:`` appear more or less often.\\n- Another significant feature involves whether the sender is affiliated with\\n  a university, as indicated either by their headers or their signature.\\n- The word \"article\" is a significant feature, based on how often people quote\\n  previous posts like this: \"In article [article ID], [name] <[e-mail address]>\\n  wrote:\"\\n- Other features match the names and e-mail addresses of particular people who\\n  were posting at the time.\\n\\nWith such an abundance of clues that distinguish newsgroups, the classifiers\\nbarely have to identify topics from text at all, and they all perform at the\\nsame high level.\\n\\nFor this reason, the functions that load 20 Newsgroups data provide a\\nparameter called **remove**, telling it what kinds of information to strip out\\nof each file. **remove** should be a tuple containing any subset of\\n``(\\'headers\\', \\'footers\\', \\'quotes\\')``, telling it to remove headers, signature\\nblocks, and quotation blocks respectively.\\n\\n  >>> newsgroups_test = fetch_20newsgroups(subset=\\'test\\',\\n  ...                                      remove=(\\'headers\\', \\'footers\\', \\'quotes\\'),\\n  ...                                      categories=categories)\\n  >>> vectors_test = vectorizer.transform(newsgroups_test.data)\\n  >>> pred = clf.predict(vectors_test)\\n  >>> metrics.f1_score(pred, newsgroups_test.target, average=\\'macro\\')\\n  0.77310...\\n\\nThis classifier lost over a lot of its F-score, just because we removed\\nmetadata that has little to do with topic classification.\\nIt loses even more if we also strip this metadata from the training data:\\n\\n  >>> newsgroups_train = fetch_20newsgroups(subset=\\'train\\',\\n  ...                                       remove=(\\'headers\\', \\'footers\\', \\'quotes\\'),\\n  ...                                       categories=categories)\\n  >>> vectors = vectorizer.fit_transform(newsgroups_train.data)\\n  >>> clf = MultinomialNB(alpha=.01)\\n  >>> clf.fit(vectors, newsgroups_train.target)\\n  MultinomialNB(alpha=0.01, class_prior=None, fit_prior=True)\\n\\n  >>> vectors_test = vectorizer.transform(newsgroups_test.data)\\n  >>> pred = clf.predict(vectors_test)\\n  >>> metrics.f1_score(newsgroups_test.target, pred, average=\\'macro\\')\\n  0.76995...\\n\\nSome other classifiers cope better with this harder version of the task. Try\\nrunning :ref:`sphx_glr_auto_examples_model_selection_grid_search_text_feature_extraction.py` with and without\\nthe ``--filter`` option to compare the results.\\n\\n.. topic:: Recommendation\\n\\n  When evaluating text classifiers on the 20 Newsgroups data, you\\n  should strip newsgroup-related metadata. In scikit-learn, you can do this by\\n  setting ``remove=(\\'headers\\', \\'footers\\', \\'quotes\\')``. The F-score will be\\n  lower because it is more realistic.\\n\\n.. topic:: Examples\\n\\n   * :ref:`sphx_glr_auto_examples_model_selection_grid_search_text_feature_extraction.py`\\n\\n   * :ref:`sphx_glr_auto_examples_text_plot_document_classification_20newsgroups.py`\\n'"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "newsgroups_train['DESCR']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((2034,), (2034,))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "newsgroups_train.filenames.shape, newsgroups_train.target.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at some of the data.  Can you guess which category these messages are in?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hi,\n",
      "\n",
      "I've noticed that if you only save a model (with all your mapping planes\n",
      "positioned carefully) to a .3DS file that when you reload it after restarting\n",
      "3DS, they are given a default position and orientation.  But if you save\n",
      "to a .PRJ file their positions/orientation are preserved.  Does anyone\n",
      "know why this information is not stored in the .3DS file?  Nothing is\n",
      "explicitly said in the manual about saving texture rules in the .PRJ file. \n",
      "I'd like to be able to read the texture rule information, does anyone have \n",
      "the format for the .PRJ file?\n",
      "\n",
      "Is the .CEL file format available from somewhere?\n",
      "\n",
      "Rych\n",
      "\n",
      "\n",
      "Seems to be, barring evidence to the contrary, that Koresh was simply\n",
      "another deranged fanatic who thought it neccessary to take a whole bunch of\n",
      "folks with him, children and all, to satisfy his delusional mania. Jim\n",
      "Jones, circa 1993.\n",
      "\n",
      "\n",
      "Nope - fruitcakes like Koresh have been demonstrating such evil corruption\n",
      "for centuries.\n",
      "\n",
      " >In article <1993Apr19.020359.26996@sq.sq.com>, msb@sq.sq.com (Mark Brader) \n",
      "\n",
      "MB>                                                             So the\n",
      "MB> 1970 figure seems unlikely to actually be anything but a perijove.\n",
      "\n",
      "JG>Sorry, _perijoves_...I'm not used to talking this language.\n",
      "\n",
      "Couldn't we just say periapsis or apoapsis?\n",
      "\n",
      " \n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\".join(newsgroups_train.data[:3]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "hint: definition of *perijove* is the point in the orbit of a satellite of Jupiter nearest the planet's center "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['alt.atheism', 'comp.graphics', 'sci.space', 'talk.religion.misc'],\n",
       "      dtype='<U18')"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(newsgroups_train.target_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['comp.graphics', 'talk.religion.misc', 'sci.space'], dtype='<U18')"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(newsgroups_train.target_names)[newsgroups_train.target[:3]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The target attribute is the integer index of the category."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 3, 2, 0, 2, 0, 2, 1, 2, 1])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "newsgroups_train.target[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_topics, num_top_words = 6, 8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stop words, stemming, lemmatization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stop words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From [Intro to Information Retrieval](https://nlp.stanford.edu/IR-book/html/htmledition/dropping-common-terms-stop-words-1.html):\n",
    "\n",
    "*Some extremely common words which would appear to be of little value in helping select documents matching a user need are excluded from the vocabulary entirely. These words are called stop words.*\n",
    "\n",
    "*The general trend in IR systems over time has been from standard use of quite large stop lists (200-300 terms) to very small stop lists (7-12 terms) to no stop list whatsoever. Web search engines generally do not use stop lists.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### NLTK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tessa/fast_ai/env/lib/python3.6/site-packages/sklearn/utils/deprecation.py:144: FutureWarning: The sklearn.feature_extraction.stop_words module is  deprecated in version 0.22 and will be removed in version 0.24. The corresponding classes / functions should instead be imported from sklearn.feature_extraction.text. Anything that cannot be imported from sklearn.feature_extraction.text is now part of the private API.\n",
      "  warnings.warn(message, FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['a',\n",
       " 'about',\n",
       " 'above',\n",
       " 'across',\n",
       " 'after',\n",
       " 'afterwards',\n",
       " 'again',\n",
       " 'against',\n",
       " 'all',\n",
       " 'almost',\n",
       " 'alone',\n",
       " 'along',\n",
       " 'already',\n",
       " 'also',\n",
       " 'although',\n",
       " 'always',\n",
       " 'am',\n",
       " 'among',\n",
       " 'amongst',\n",
       " 'amoungst']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction import stop_words\n",
    "\n",
    "sorted(list(stop_words.ENGLISH_STOP_WORDS))[:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is no single universal list of stop words."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stemming and Lemmatization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "from [Information Retrieval](https://nlp.stanford.edu/IR-book/html/htmledition/stemming-and-lemmatization-1.html) textbook:\n",
    "\n",
    "Are the below words the same?\n",
    "\n",
    "*organize, organizes, and organizing*\n",
    "\n",
    "*democracy, democratic, and democratization*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stemming and Lemmatization both generate the root form of the words. \n",
    "\n",
    "Lemmatization uses the rules about a language.  The resulting tokens are all actual words\n",
    "\n",
    "\"Stemming is the poor-man’s lemmatization.\" (Noah Smith, 2011) Stemming is a crude heuristic that chops the ends off of words.  The resulting tokens may not be actual words. Stemming is faster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /home/tessa/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import stem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "wnl = stem.WordNetLemmatizer()\n",
    "porter = stem.porter.PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_list = ['feet', 'foot', 'foots', 'footing']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['foot', 'foot', 'foot', 'footing']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[wnl.lemmatize(word) for word in word_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['feet', 'foot', 'foot', 'foot']"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[porter.stem(word) for word in word_list]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your turn!  Now, try lemmatizing and stemming the following collections of words:\n",
    "\n",
    "- fly, flies, flying\n",
    "- organize, organizes, organizing\n",
    "- universe, university"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['fly', 'fly', 'flying']\n",
      "['fli', 'fli', 'fli']\n",
      "['organize', 'organizes', 'organizing']\n",
      "['organ', 'organ', 'organ']\n",
      "['universe', 'university']\n",
      "['univers', 'univers']\n"
     ]
    }
   ],
   "source": [
    "fly_words = ['fly','flies','flying']\n",
    "org_words = ['organize','organizes','organizing']\n",
    "uni_words = ['universe','university']\n",
    "\n",
    "\n",
    "def print_lem_stem_words(word_lst):\n",
    "    print([wnl.lemmatize(word) for word in word_lst])\n",
    "    print([porter.stem(word) for word in word_lst])\n",
    "    \n",
    "print_lem_stem_words(fly_words)\n",
    "print_lem_stem_words(org_words)\n",
    "print_lem_stem_words(uni_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "fastai/course-nlp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stemming and lemmatization are language dependent.  Languages with more complex morphologies may show bigger benefits.  For example, Sanskrit has a very [large number of verb forms](https://en.wikipedia.org/wiki/Sanskrit_verbs). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spacy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stemming and lemmatization are implementation dependent."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Spacy is a very modern & fast nlp library. Spacy is opinionated, in that it typically offers one highly optimized way to do something (whereas nltk offers a huge variety of ways, although they are usually not as optimized).\n",
    "\n",
    "You will need to install it.\n",
    "\n",
    "if you use conda:\n",
    "```\n",
    "conda install -c conda-forge spacy\n",
    "```\n",
    "if you use pip:\n",
    "```\n",
    "pip install -U spacy\n",
    "```\n",
    "\n",
    "You will then need to download the English model:\n",
    "```\n",
    "spacy -m download en_core_web_sm\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy.lemmatizer import Lemmatizer\n",
    "from spacy.lookups import Lookups\n",
    "lookups = Lookups()\n",
    "# lookups.add_table(\"lemma_rules\", {\"noun\": [[\"s\", \"\"]]})\n",
    "\n",
    "lemmatizer = Lemmatizer(lookups)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['feet', 'foot', 'foots', 'footing']"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[lemmatizer.lookup(word) for word in word_list]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Spacy doesn't offer a stemmer (since lemmatization is considered better-- this is an example of being opinionated!)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stop words vary from library to library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"'d\",\n",
       " \"'ll\",\n",
       " \"'m\",\n",
       " \"'re\",\n",
       " \"'s\",\n",
       " \"'ve\",\n",
       " 'a',\n",
       " 'about',\n",
       " 'above',\n",
       " 'across',\n",
       " 'after',\n",
       " 'afterwards',\n",
       " 'again',\n",
       " 'against',\n",
       " 'all',\n",
       " 'almost',\n",
       " 'alone',\n",
       " 'along',\n",
       " 'already',\n",
       " 'also']"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted(list(nlp.Defaults.stop_words))[:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise: What stop words appear in spacy but not in sklearn?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Exercise:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{\"'d\",\n",
       " \"'ll\",\n",
       " \"'m\",\n",
       " \"'re\",\n",
       " \"'s\",\n",
       " \"'ve\",\n",
       " 'ca',\n",
       " 'did',\n",
       " 'does',\n",
       " 'doing',\n",
       " 'just',\n",
       " 'make',\n",
       " \"n't\",\n",
       " 'n‘t',\n",
       " 'n’t',\n",
       " 'quite',\n",
       " 'really',\n",
       " 'regarding',\n",
       " 'say',\n",
       " 'unless',\n",
       " 'used',\n",
       " 'using',\n",
       " 'various',\n",
       " '‘d',\n",
       " '‘ll',\n",
       " '‘m',\n",
       " '‘re',\n",
       " '‘s',\n",
       " '‘ve',\n",
       " '’d',\n",
       " '’ll',\n",
       " '’m',\n",
       " '’re',\n",
       " '’s',\n",
       " '’ve'}"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp.Defaults.stop_words - stop_words.ENGLISH_STOP_WORDS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "#### Exercise: And what stop words are in sklearn but not spacy?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "frozenset({'amoungst',\n",
       "           'bill',\n",
       "           'cant',\n",
       "           'co',\n",
       "           'con',\n",
       "           'couldnt',\n",
       "           'cry',\n",
       "           'de',\n",
       "           'describe',\n",
       "           'detail',\n",
       "           'eg',\n",
       "           'etc',\n",
       "           'fill',\n",
       "           'find',\n",
       "           'fire',\n",
       "           'found',\n",
       "           'hasnt',\n",
       "           'ie',\n",
       "           'inc',\n",
       "           'interest',\n",
       "           'ltd',\n",
       "           'mill',\n",
       "           'sincere',\n",
       "           'system',\n",
       "           'thick',\n",
       "           'thin',\n",
       "           'un'})"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Exercise:\n",
    "stop_words.ENGLISH_STOP_WORDS - nlp.Defaults.stop_words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### When to use these?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "<img src=\"images/skomoroch.png\" alt=\"\" style=\"width: 65%\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "These were long considered standard techniques, but they can often **hurt** your performance **if using deep learning**. Stemming, lemmatization, and removing stop words all involve throwing away information.\n",
    "\n",
    "However, they can still be useful when working with simpler models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Another approach: sub-word units"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "[SentencePiece](https://github.com/google/sentencepiece) library from Google"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, scikit learn has a method that will extract all the word counts for us.  In the next lesson, we'll learn how to write our own version of CountVectorizer, to see what's happening underneath the hood."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "# nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from nltk import word_tokenize\n",
    "\n",
    "# class LemmaTokenizer(object):\n",
    "#     def __init__(self):\n",
    "#         self.wnl = stem.WordNetLemmatizer()\n",
    "#     def __call__(self, doc):\n",
    "#         return [self.wnl.lemmatize(t) for t in word_tokenize(doc)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer(stop_words='english') #, tokenizer=LemmaTokenizer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "matrix([[0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        ...,\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0]])"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectors = vectorizer.fit_transform(newsgroups_train.data).todense() # (documents, vocab)\n",
    "vectors.shape #, vectors.nnz / vectors.shape[0], row_means.shape\n",
    "vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2034 (2034, 26576)\n"
     ]
    }
   ],
   "source": [
    "print(len(newsgroups_train.data), vectors.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = np.array(vectorizer.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(26576,)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['cosmonauts', 'cosmos', 'cosponsored', 'cost', 'costa', 'costar',\n",
       "       'costing', 'costly', 'costruction', 'costs', 'cosy', 'cote',\n",
       "       'couched', 'couldn', 'council', 'councils', 'counsel',\n",
       "       'counselees', 'counselor', 'count'], dtype='<U80')"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab[7000:7020]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Singular Value Decomposition (SVD)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"SVD is not nearly as famous as it should be.\" - Gilbert Strang"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We would clearly expect that the words that appear most frequently in one topic would appear less frequently in the other - otherwise that word wouldn't make a good choice to separate out the two topics. Therefore, we expect the topics to be **orthogonal**.\n",
    "\n",
    "The SVD algorithm factorizes a matrix into one matrix with **orthogonal columns** and one with **orthogonal rows** (along with a diagonal matrix, which contains the **relative importance** of each factor).\n",
    "\n",
    "<img src=\"images/svd_fb.png\" alt=\"\" style=\"width: 80%\"/>\n",
    "(source: [Facebook Research: Fast Randomized SVD](https://research.fb.com/fast-randomized-svd/))\n",
    "\n",
    "SVD is an **exact decomposition**, since the matrices it creates are big enough to fully cover the original matrix. SVD is extremely widely used in linear algebra, and specifically in data science, including:\n",
    "\n",
    "- semantic analysis\n",
    "- collaborative filtering/recommendations ([winning entry for Netflix Prize](https://datajobs.com/data-science-repo/Recommender-Systems-%5BNetflix%5D.pdf))\n",
    "- calculate Moore-Penrose pseudoinverse\n",
    "- data compression\n",
    "- principal component analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Latent Semantic Analysis (LSA) uses SVD.  You will sometimes hear topic modelling referred to as LSA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 5min 20s, sys: 9min 16s, total: 14min 36s\n",
      "Wall time: 49.7 s\n"
     ]
    }
   ],
   "source": [
    "#vectors is the original term-document matrix with counts for non-stop words\n",
    "%time U, s, Vh = linalg.svd(vectors, full_matrices=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2034, 2034) (2034,) (2034, 26576)\n"
     ]
    }
   ],
   "source": [
    "print(U.shape, s.shape, Vh.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Confirm this is a decomposition of the input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([433.92698542, 291.51012741, 240.71137677, 220.00048043])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s[:4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([433.92698542, 291.51012741, 240.71137677, 220.00048043])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.diag(np.diag(s[:4]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Exercise: confrim that U, s, Vh is a decomposition of `vectors`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Confirm that U, V are orthonormal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "#### Answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#Exercise: Confirm that U, Vh are orthonormal\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "#### Topics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "What can we say about the singular values s?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2034,)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(s);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7fae77be4128>]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(s[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "num_top_words=10\n",
    "\n",
    "def show_topics(a):\n",
    "    top_words = lambda t: [vocab[i] for i in np.argsort(t)[:-num_top_words-1:-1]]\n",
    "    topic_words = ([top_words(t) for t in a])\n",
    "    return [' '.join(t) for t in topic_words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ditto critus propagandist surname galacticentric kindergarten surreal imaginative salvadorans autotheism',\n",
       " 'jpeg gif file color quality image jfif format bit version',\n",
       " 'graphics edu pub mail 128 3d ray ftp send image',\n",
       " 'jesus god matthew people atheists atheism does graphics religious said',\n",
       " 'image data processing analysis software available tools display tool user',\n",
       " 'god atheists atheism religious believe religion argument true atheist example',\n",
       " 'space nasa lunar mars probe moon missions probes surface earth',\n",
       " 'image probe surface lunar mars probes moon orbit mariner mission',\n",
       " 'argument fallacy conclusion example true ad argumentum premises false valid',\n",
       " 'space larson image theory universe physical nasa material star unified']"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_topics(Vh[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "We get topics that match the kinds of clusters we would expect! This is despite the fact that this is an **unsupervised algorithm** - which is to say, we never actually told the algorithm how our documents are grouped."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "We will return to SVD in **much more detail** later.  For now, the important takeaway is that we have a tool that allows us to exactly factor a matrix into orthogonal columns and orthogonal rows."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Non-negative Matrix Factorization (NMF)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Motivation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/face_pca.png\" alt=\"PCA on faces\" style=\"width: 80%\"/>\n",
    "\n",
    "(source: [NMF Tutorial](http://perso.telecom-paristech.fr/~essid/teach/NMF_tutorial_ICME-2014.pdf))\n",
    "\n",
    "A more interpretable approach:\n",
    "\n",
    "<img src=\"images/face_outputs.png\" alt=\"NMF on Faces\" style=\"width: 80%\"/>\n",
    "\n",
    "(source: [NMF Tutorial](http://perso.telecom-paristech.fr/~essid/teach/NMF_tutorial_ICME-2014.pdf))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Idea"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rather than constraining our factors to be *orthogonal*, another idea would to constrain them to be *non-negative*. NMF is a factorization of a non-negative data set $V$: $$ V = W H$$ into non-negative matrices $W,\\; H$. Often positive factors will be **more easily interpretable** (and this is the reason behind NMF's popularity). \n",
    "\n",
    "<img src=\"images/face_nmf.png\" alt=\"NMF on faces\" style=\"width: 80%\"/>\n",
    "\n",
    "(source: [NMF Tutorial](http://perso.telecom-paristech.fr/~essid/teach/NMF_tutorial_ICME-2014.pdf))\n",
    "\n",
    "Nonnegative matrix factorization (NMF) is a non-exact factorization that factors into one skinny positive matrix and one short positive matrix.  NMF is NP-hard and non-unique.  There are a number of variations on it, created by adding different constraints. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Applications of NMF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- [Face Decompositions](http://scikit-learn.org/stable/auto_examples/decomposition/plot_faces_decomposition.html#sphx-glr-auto-examples-decomposition-plot-faces-decomposition-py)\n",
    "- [Collaborative Filtering, eg movie recommendations](http://www.quuxlabs.com/blog/2010/09/matrix-factorization-a-simple-tutorial-and-implementation-in-python/)\n",
    "- [Audio source separation](https://pdfs.semanticscholar.org/cc88/0b24791349df39c5d9b8c352911a0417df34.pdf)\n",
    "- [Chemistry](http://ieeexplore.ieee.org/document/1532909/)\n",
    "- [Bioinformatics](https://bmcbioinformatics.biomedcentral.com/articles/10.1186/s12859-015-0485-4) and [Gene Expression](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC2623306/)\n",
    "- Topic Modeling (our problem!)\n",
    "\n",
    "<img src=\"images/nmf_doc.png\" alt=\"NMF on documents\" style=\"width: 80%\"/>\n",
    "\n",
    "(source: [NMF Tutorial](http://perso.telecom-paristech.fr/~essid/teach/NMF_tutorial_ICME-2014.pdf))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**More Reading**:\n",
    "\n",
    "- [The Why and How of Nonnegative Matrix Factorization](https://arxiv.org/pdf/1401.5226.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NMF from sklearn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use [scikit-learn's implementation of NMF](http://scikit-learn.org/stable/modules/generated/sklearn.decomposition.NMF.html):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "m,n=vectors.shape\n",
    "d=5  # num topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = decomposition.NMF(n_components=d, random_state=1)\n",
    "\n",
    "W1 = clf.fit_transform(vectors)\n",
    "H1 = clf.components_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['jpeg image gif file color images format quality version files',\n",
       " 'edu graphics pub mail 128 ray ftp send 3d com',\n",
       " 'space launch satellite nasa commercial satellites year market data earth',\n",
       " 'jesus god people matthew atheists does atheism said just believe',\n",
       " 'image data available software processing ftp edu analysis images display']"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_topics(H1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TF-IDF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Topic Frequency-Inverse Document Frequency](http://www.tfidf.com/) (TF-IDF) is a way to normalize term counts by taking into account how often they appear in a document, how long the document is, and how commmon/rare the term is.\n",
    "\n",
    "TF = (# occurrences of term t in document) / (# of words in documents)\n",
    "\n",
    "IDF = log(# of documents / # documents with term t in it)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<2034x26576 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 133634 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer_tfidf = TfidfVectorizer(stop_words='english')\n",
    "vectors_tfidf = vectorizer_tfidf.fit_transform(newsgroups_train.data) # (documents, vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"a\\n\\nWhat about positional uncertainties in S-L 1993e?   I assume we know where\\nand what Galileo is doing within a few meters.   But without the\\nHGA,  don't we have to have some pretty good ideas, of where to look\\nbefore imaging?  If the HGA was working,  they could slew around\\nin near real time (Less speed of light delay).  But when they were\\nimaging toutatis????  didn't someone have to get lucky on a guess to\\nfind the first images?   \\n\\nAlso, I imagine S-L 1993e will be mostly a visual image.  so how will\\nthat affect the other imaging missions.  with the LGA,  there is a real\\ntight allocation of bandwidth.   It may be premature to hope for answers,\\nbut I thought i'd throw it on the floor.\",\n",
       " \"I would like to program Tseng ET4000 to nonstandard 1024x768 mode by\\nswitching to standard 1024x768 mode using BIOS and than changing some\\ntiming details (0x3D4 registers 0x00-0x1F) but I don't know how to\\nselect 36 MHz pixel clock I need. The BIOS function selects 40 MHz.\\n\\nIs there anybody who knows where to obtain technical info about this.\\nI am also interested in any other technical information about Tseng ET4000\\nand Trident 8900 and 9000 chipsets.\\n\\n\\t\\t\\tthanks very much\",\n",
       " 'In-Reply-To: <20APR199312262902@rigel.tamu.edu> lmp8913@rigel.tamu.edu (PRESTON, LISA M)',\n",
       " \"\\n\\n\\n\\nI'm not sure, but it almost sounds like they can't figure out where the \\n_nucleus_ is within the coma. If they're off by a couple hundred\\nmiles, well, you can imagine the rest...\\n\",\n",
       " \"Hello,\\n     I am looking to add voice input capability to a user interface I am\\ndeveloping on an HP730 (UNIX) workstation.  I would greatly appreciate \\ninformation anyone would care to offer about voice input systems that are \\neasily accessible from the UNIX environment. \\n\\n     The names or adresses of applicable vendors, as well as any \\nexperiences you have had with specific systems, would be very helpful.\\n\\n     Please respond via email; I will post a summary if there is \\nsufficient interest.\\n\\n\\nThanks,\\nKen\\n\\n\\nP.S.  I have found several impressive systems for IBM PC's, but I would \\nlike to avoid the hassle of purchasing and maintaining a separate PC if \\nat all possible.\\n\\n-------------------------------------------------------------------------------\\nKen Hinckley (kph2q@virginia.edu)\\nUniversity of Virginia \\nNeurosurgical Visualization Laboratory\",\n",
       " '\\nIt was a test of the first reusable tool.\\n\\n\\nPointy so they can find them or so they will stick into their pants better, and\\nbe closer to their brains?',\n",
       " '\\nSize of armies, duration, numbers of casualties both absolute and as a\\npercentage of those involved, geographical area and numbers of countries\\ntoo, are all measures of size.  In this case I\\'d say the relevant\\nstatistic would be the number of combatants (total troops) compared to\\ntotal casualties from among the total civilian population in the\\naffected geographical area.\\n\\n\\nVietnam and Korea might make good comparisons.\\n\\n\\nWestern news in general, but in particular the American \"mass media\":\\nCBS, NBC, ABC, etc.  The general tone of the news during the whole\\nwar was one of \"those poor, poor Iraqis\" along with \"look how precisely\\nthis cruise missile blew this building to bits\".\\n\\n\\nI agree.\\n\\n\\nPerhaps so.  And maybe the atomic bomb was a mistake too.  But that\\'s easy\\nto say from our \"enlightened\" viewpoint here in the 90\\'s, right?  Back\\nthen, it was *all-out* war, and Germany and Japan had to be squashed.\\nAfter all, a million or more British had already died, hundreds of \\nthousands of French, a couple hundread thousand or so Americans, and \\nmillions of Russians, not to mention a few million Jews, Poles, and \\nother people of slavic descent in German concentration camps.  All \\nthings considered, the fire-bombings and the atomic bomb were\\nessential (and therefore justified) in bringing the war to a quick\\nend to avoid even greater allied losses.\\n\\nI, for one, don\\'t regret it.\\n\\n\\nSure.  And it\\'s the people who suffer because of them.  All the more\\nreason to depose these \"entrenched political rulers operating in their\\nown selfish interests\"!  Or do you mean that this applies to the allies\\nas well??\\n\\n\\nI make no claim or effort to justify the misguided foreign policy of the\\nWest before the war.  It is evident that the West, especially America,\\nmisjudged Hussein drastically.  But once Hussein invaded Kuwait and \\nthreatened to militarily corner a significant portion of the world\\'s\\noil supply, he had to be stopped.  Sure the war could have been\\nprevented by judicious and concerted effort on the part of the West\\nbefore Hussein invaded Kuwait, but it is still *Hussein* who is\\nresponsible for his decision to invade.  And once he did so, a\\nstrong response from the West was required.\\n\\n\\nWell, it\\'s not very \"loving\" to allow a Hussein or a Hitler to gobble up\\nnearby countries and keep them.  Or to allow them to continue with mass\\nslaughter of certain peoples under their dominion.  So, I\\'d have to\\nsay yes, stopping Hussein was the most \"loving\" thing to do for the\\nmost people involved once he set his mind on military conquest.\\n\\nI mentioned it.\\n\\nIf we hadn\\'t intervened, allowing Hussein to keep Kuwait, then it would\\nhave been appeasement.  It is precisely the lessons the world learned\\nin WW2 that motivated the Western alliance to war.  Letting Hitler take\\nAustria and Czechoslavkia did not stop WW2 from happening, and letting\\nHussein keep Kuwait would not have stopped an eventual Gulf War to\\nprotect Saudi Arabia.\\n\\n\\nSure.  What was truly unfortunate was that they followed Hitler in\\nhis grandiose quest for a \"Thousand Year Reich\".  The consequences\\nstemmed from that.\\n\\nWhat should I say about them?  Anything in particular?\\n\\n\\n\\nSo?  It was the *policemen* on trial not Rodney King!!  And under American\\nlaw they deserved a jury of *their* peers!  If there had been black\\nofficers involved, I\\'m sure their would have been black jurors too.\\nThis point (of allegedly racial motivations) is really shallow.\\n\\n\\nSo?  It\\'s \"hard to imagine\"?  So when has Argument from Incredulity\\ngained acceptance from the revered author of \"Constructing a Logical\\nArgument\"?  Can we expect another revision soon??  :)  (Just kidding.)\\n\\n\\nI have to admit that I wonder this too.  But *neither* the prosecution\\nnor the defense is talking.  So one cannot conclude either way due to\\nthe silence of the principals.  \\n\\n\\nOK.  It certainly seemed to me that there was excessive force involved.\\nAnd frankly, the original \"not guilty\" verdict baffled me too.  But then\\nI learned that the prosecution in the first case did not try to convict\\non a charge of excessive force or simple assault which they probably\\nwould have won, they tried to get a conviction on a charge of aggravated\\nassault with intent to inflict serious bodily harm.  A charge, which\\nnews commentators said, was akin to attempted murder under California\\nlaw.  Based on what the prosecution was asking for, it\\'s evident that \\nthe first jury decided that the officers were \"not guilty\".  Note, \\nnot \"not guilty\" of doing wrong, but \"not guilty\" of aggravated assault \\nwith the *intent* of inflicting serious bodily harm.  The seeds of the \\nprosecutions defeat were in their own overconfidence in obtaining a \\nverdict such that they went for the most extreme charge they could.\\n\\nIf the facts as the news commentators presented them are true, then\\nI feel the \"not guilty\" verdict was a reasonable one.\\n\\n\\nThanks mathew, I like the quote.  Pretty funny actually.  (I\\'m a \\nMonty Python fan, you know.  Kind of seems in that vein.)\\n\\nOf course, oversimplifying any moral argument can make it seem\\ncontradictory.  But then, you know that already.  \\n\\nRegards,',\n",
       " \"<stuff deleted>\\n\\nYou mean like: seconds, minutes, hours, days, months, years. . .  :-)\\n\\nRemember,  the Fahrenheit temperature scale is also a centigrade scale.  Some\\nrevisionists tell the history something like this:  The coldest point in a\\nparticular Russian winter was marked on the thermometer as was the body\\ntemperature of a volunteer (turns out he was sick, but you can't win 'em all).\\nThen the space in between the marks on the thermometer was then divided into\\nhundredths.\\n\\t\\t\\t\\t\\t\\t\\t\\t:-)\\n\\nFWIW,\\n\\nDoug Page\\n\",\n",
       " \"\\nIt wasn't especially prominent, as I recall.  However, quite possibly it's\\nno longer on display; NASM, like most museums, has much more stuff than it\\ncan display at once, and does rotate the displays occasionally.\",\n",
       " \"DM> Fact or rumor....?  Madalyn Murray O'Hare an atheist who eliminated the\\nDM> use of the bible reading and prayer in public schools 15 years ago is now\\nDM> going to appear before the FCC with a petition to stop the reading of the\\nDM> Gospel on the airways of America.  And she is also campaigning to remove\\nDM> Christmas programs, songs, etc from the public schools.  If it is true\\nDM> then mail to Federal Communications Commission 1919 H Street Washington DC\\nDM> 20054 expressing your opposition to her request.  Reference Petition number\\n\\nDM> 2493.\\n\\nFalse.  This story has been going around for years.  There's not a drop of\\ntruth.  Note that I don't care for O'Hare (O'Hair?) myself, but this\\nis one thing she's not guilty of.\\n\"]"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "newsgroups_train.data[10:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "W1 = clf.fit_transform(vectors_tfidf)\n",
    "H1 = clf.components_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['people don think just like objective say morality know moral',\n",
       " 'graphics thanks files image file program windows know format ftp',\n",
       " 'space nasa launch shuttle orbit moon lunar earth station data',\n",
       " 'ico bobbe tek beauchaine bronx manhattan sank queens vice blew',\n",
       " 'god jesus bible believe christian atheism does belief faith christians']"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_topics(H1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7fae5d466ba8>]"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(clf.components_[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "43.71292605795277"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.reconstruction_err_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NMF in summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Benefits: Fast and easy to use!\n",
    "\n",
    "Downsides: took years of research and expertise to create"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notes:\n",
    "- For NMF, matrix needs to be at least as tall as it is wide, or we get an error with fit_transform\n",
    "- Can use df_min in CountVectorizer to only look at words that were in at least k of the split texts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Truncated SVD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We saved a lot of time when we calculated NMF by only calculating the subset of columns we were interested in. Is there a way to get this benefit with SVD? Yes there is! It's called truncated SVD.  We are just interested in the vectors corresponding to the **largest** singular values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/svd_fb.png\" alt=\"\" style=\"width: 80%\"/>\n",
    "\n",
    "(source: [Facebook Research: Fast Randomized SVD](https://research.fb.com/fast-randomized-svd/))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Shortcomings of classical algorithms for decomposition:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Matrices are \"stupendously big\"\n",
    "- Data are often **missing or inaccurate**.  Why spend extra computational resources when imprecision of input limits precision of the output?\n",
    "- **Data transfer** now plays a major role in time of algorithms.  Techniques the require fewer passes over the data may be substantially faster, even if they require more flops (flops = floating point operations).\n",
    "- Important to take advantage of **GPUs**.\n",
    "\n",
    "(source: [Halko](https://arxiv.org/abs/0909.4061))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Advantages of randomized algorithms:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- inherently stable\n",
    "- performance guarantees do not depend on subtle spectral properties\n",
    "- needed matrix-vector products can be done in parallel\n",
    "\n",
    "(source: [Halko](https://arxiv.org/abs/0909.4061))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Timing comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 5min 32s, sys: 10min, total: 15min 32s\n",
      "Wall time: 56.8 s\n"
     ]
    }
   ],
   "source": [
    "%time u, s, v = np.linalg.svd(vectors, full_matrices=False)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "from sklearn import decomposition\n",
    "import fbpca"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%time u, s, v = decomposition.randomized_svd(vectors, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Randomized SVD from Facebook's library fbpca:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 26 s, sys: 644 ms, total: 26.7 s\n",
      "Wall time: 1.19 s\n"
     ]
    }
   ],
   "source": [
    "%time u, s, v = fbpca.pca(vectors, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For more on randomized SVD, check out my [PyBay 2017 talk](https://www.youtube.com/watch?v=7i6kBz1kZ-A&list=PLtmWHNX-gukLQlMvtRJ19s7-8MrnRV6h6&index=7).\n",
    "\n",
    "For significantly more on randomized SVD, check out the [Computational Linear Algebra course](https://github.com/fastai/numerical-linear-algebra)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## End"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
