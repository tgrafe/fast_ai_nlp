{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Due on: 5/30.  Please upload your completed assignment to Canvas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. For the following words: logistic, logistics, shoe, shoes\n",
    "\n",
    "   a. Porter stem with nltk\n",
    "   \n",
    "   b. lemmatize with nltk\n",
    "   \n",
    "   c. lemmatize with Spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "import pandas as pd\n",
    "from scipy import linalg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "UsageError: Line magic function `%matplotlib_inline` not found.\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib_inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import decomposition\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['logist', 'logist', 'shoe', 'shoe']\n",
      "['logistic', 'logistics', 'shoe', 'shoe']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[['logistic'], ['logistics'], ['shoe'], ['shoes']]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_list = ['logistic','logistics','shoe','shoes']\n",
    "\n",
    "porter = nltk.stem.porter.PorterStemmer()\n",
    "print([porter.stem(w) for w in word_list])\n",
    "\n",
    "lemmatizer = nltk.stem.WordNetLemmatizer()\n",
    "print([lemmatizer.lemmatize(w) for w in word_list])\n",
    "\n",
    "# lookups = spacy.lookups.Lookups()\n",
    "# splemmatizer = spacy.lemmatizer.Lemmatizer(lookups)\n",
    "# splemmatizer('hello')\n",
    "# print([splemmatizer(w) for w in word_list])\n",
    "\n",
    "from spacy.lemmatizer import Lemmatizer\n",
    "from spacy.lookups import Lookups\n",
    "lookups = Lookups()\n",
    "# lookups.add_table(\"lemma_rules\", {\"noun\": [[\"s\", \"\"]]})\n",
    "\n",
    "lemmatizer = Lemmatizer(lookups)\n",
    "[lemmatizer(w, 1) for w in word_list]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2\\. n-grams are an important NLP concept.  An n-gram is a contiguous sequence of n items (where the items can be characters, syllables, or words).  Here, we  A 1-gram is a unigram, a 2-gram is a bigram, and a 3-gram is a trigram.\n",
    "\n",
    "Here, we are referring to sequences of words. The sentence \"It was a bright cold day in April.\" contains the following trigrams:\n",
    "\n",
    "- It was a\n",
    "- was a bright\n",
    "- a bright cold\n",
    "- bright cold day\n",
    "- cold day in\n",
    "- day in April\n",
    "\n",
    "Write a function that returns a dictionary with the n-grams of a text (for `min_n <= n <= max_n`) and a count of how often they appear:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'I like': 3,\n",
       " 'like fish': 2,\n",
       " 'fish and': 1,\n",
       " 'and I': 2,\n",
       " 'like chicken': 1,\n",
       " 'chicken and': 1,\n",
       " 'I like fish': 2,\n",
       " 'like fish and': 1,\n",
       " 'fish and I': 1,\n",
       " 'and I like': 2,\n",
       " 'I like chicken': 1,\n",
       " 'like chicken and': 1,\n",
       " 'chicken and I': 1}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_ngrams(text, min_n, max_n):\n",
    "    \n",
    "    #Exercise: FILL IN METHOD\n",
    "    doc = nlp(text)\n",
    "    words = [str(d) for d in doc if not d.is_punct]\n",
    "    ngram_dict = {}\n",
    "    \n",
    "    for n in range(min_n, max_n + 1):\n",
    "        for w in range(len(words)-n+1):\n",
    "            ngram = \" \".join(words[w:w+n])\n",
    "            if ngram in ngram_dict.keys():\n",
    "                ngram_dict[ngram] += 1\n",
    "            else:\n",
    "                ngram_dict[ngram] = 1\n",
    "            \n",
    "    return ngram_dict\n",
    "\n",
    "text = \"I like fish, and I like chicken and I like fish.\"\n",
    "get_ngrams(text, 2, 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3\\. Write a method that given a list of strings (you can think of each string as a document), returns a dictionary for each string, where the keys are the vocabulary words, and the values are the frequencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'hello': 2, 'how': 1, 'are': 2, 'you': 2, 'ok': 1},\n",
       " {'la': 4},\n",
       " {'do': 1, 'ray': 1, 'me': 1, 'fa': 1, 'so': 1, 'la': 1, 'ti': 1, 'doh': 1}]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_vocab_frequency(list_of_strings):\n",
    "    ngram_dicts = []\n",
    "    for text in list_of_strings:\n",
    "        ngram_dict = get_ngrams(text, 1, 1)\n",
    "        ngram_dicts.append(ngram_dict)\n",
    "    return ngram_dicts\n",
    "\n",
    "get_vocab_frequency([\"hello! hello? how are you? are you ok?\",\"la la la la\",\"do ray me fa so la ti doh\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4\\. Write a method that when given a list of strings (you can think of each string as a document), calculates the TF-IDF, and returns a term-document matrix with the results. It will be useful to use your `get_vocab_frequency` method from problem 3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lst = ['a','b','a']\n",
    "lst.count('a')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.916290731874155"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "np.log(0.4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>hello</th>\n",
       "      <th>your</th>\n",
       "      <th>how</th>\n",
       "      <th>my</th>\n",
       "      <th>ray</th>\n",
       "      <th>are</th>\n",
       "      <th>la</th>\n",
       "      <th>fa</th>\n",
       "      <th>you</th>\n",
       "      <th>cat</th>\n",
       "      <th>ok</th>\n",
       "      <th>do</th>\n",
       "      <th>so</th>\n",
       "      <th>best</th>\n",
       "      <th>the</th>\n",
       "      <th>ti</th>\n",
       "      <th>is</th>\n",
       "      <th>doh</th>\n",
       "      <th>me</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.183102</td>\n",
       "      <td>0.091551</td>\n",
       "      <td>0.183102</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.183102</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.183102</td>\n",
       "      <td>0.033789</td>\n",
       "      <td>0.091551</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.033789</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.219722</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.081093</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.219722</td>\n",
       "      <td>0.219722</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.081093</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.137327</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.137327</td>\n",
       "      <td>0.137327</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.137327</td>\n",
       "      <td>0.137327</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.137327</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.137327</td>\n",
       "      <td>0.137327</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      hello      your       how        my       ray       are        la  \\\n",
       "0  0.183102  0.091551  0.183102  0.000000  0.000000  0.183102  0.000000   \n",
       "1  0.000000  0.000000  0.000000  0.219722  0.000000  0.000000  0.000000   \n",
       "2  0.000000  0.000000  0.000000  0.000000  0.137327  0.000000  0.137327   \n",
       "\n",
       "         fa       you       cat        ok        do        so      best  \\\n",
       "0  0.000000  0.183102  0.033789  0.091551  0.000000  0.000000  0.000000   \n",
       "1  0.000000  0.000000  0.081093  0.000000  0.000000  0.000000  0.219722   \n",
       "2  0.137327  0.000000  0.000000  0.000000  0.137327  0.137327  0.000000   \n",
       "\n",
       "        the        ti        is       doh        me  \n",
       "0  0.000000  0.000000  0.033789  0.000000  0.000000  \n",
       "1  0.219722  0.000000  0.081093  0.000000  0.000000  \n",
       "2  0.000000  0.137327  0.000000  0.137327  0.137327  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TF(t) = (Number of times term t appears in a document) / (Total number of terms in the document).\n",
    "# IDF(t) = log_e(Total number of documents / Number of documents with term t in it)\n",
    "def get_tfidf(list_of_strings):\n",
    "    vocab = get_vocab_frequency(list_of_strings)\n",
    "    all_words = []\n",
    "    for v in vocab:\n",
    "        all_words.extend(v.keys())\n",
    "    all_words = list(set(all_words))\n",
    "    matrix = {word:[] for word in all_words}\n",
    "\n",
    "    for i, v in enumerate(vocab):\n",
    "        word_count = sum([freq for key,freq in v.items()])\n",
    "        doc_tf = {word:(freq/word_count) for word,freq in v.items()}\n",
    "        doc_idf = {word:np.log(len(vocab)/len([doc for doc in vocab if word in doc.keys()])) for word,freq in v.items()}\n",
    "        doc_tfidf = {word:tf * doc_idf[word] for word, tf in doc_tf.items()}\n",
    "        \n",
    "        for word in matrix.keys():\n",
    "            if word in doc_tfidf.keys():\n",
    "                matrix[word].append(doc_tfidf[word])\n",
    "            else:\n",
    "                matrix[word].append(0)\n",
    "    df_matrix = pd.DataFrame(matrix)\n",
    "    return df_matrix\n",
    "\n",
    "get_tfidf([\"hello! hello? how are you? are you ok? how is your cat?\",\"my cat is the best!\",\"do ray me fa so la ti doh\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5\\. Who said the following (*Hint: Be sure to read the class notebooks and relevant links*):\n",
    "\n",
    "A. \"It's true there's been a lot of work on trying to apply statistical models to various linguistic problems. I think there have been some successes, but a lot of failures. There is a notion of success ... which I think is novel in the history of science. It interprets success as approximating unanalyzed data.\"\n",
    "chomsky\n",
    "\n",
    "B. \"I agree that it can be difficult to make sense of a model containing billions of parameters. Certainly a human can't understand such a model by inspecting the values of each parameter individually. But one can gain insight by examing the properties of the model—where it succeeds and fails, how well it learns as a function of data, etc.\"\n",
    "peter norvig\n",
    "\n",
    "norvig also said:\n",
    "Einstein said to make everything as simple as possible, but no simpler. Many phenomena in science are stochastic, and the simplest model of them is a probabilistic model; I believe language is such a phenomenon and therefore that probabilistic models are our best tool for representing facts about language, for algorithmically processing language, and for understanding how humans process language.\n",
    "\n",
    "C. The big-data big-compute paradigm of modern Deep Learning has in fact “perverted the field” (of computational linguistics) and “sent it off-track”\n",
    "chris manning\n",
    "\n",
    "D. Language is crucial to general intelligence, because language is the conduit by which individual intelligence is shared and transformed into societal intelligence.\n",
    "chris manning\n",
    "\n",
    "E. Structure is a “necessary evil”, and warned that imposing structure requires us to make certain assumptions, which are invariably wrong for at least some portion of the data, and may become obsolete within the near future.\n",
    "lecun"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_term_doc_test_train_sets(filepath, train_set_size, col = 'Name',):\n",
    "    df = pd.read_csv(filepath)\n",
    "    df = df[[col,\"Text\"]]\n",
    "    target_names = df.Name.unique().tolist()\n",
    "    df[\"target\"] = df.apply(lambda x: target_names.index(x.Name), axis = 1)\n",
    "    df = df.rename(columns = {'Text':'data'})\n",
    "    df = df[[\"target\",\"data\"]]\n",
    "    shuffled = df.sample(frac =1)\n",
    "    train_set = {'data':shuffled[0:train_set_size].data.tolist(), 'target_names':target_names, 'target':shuffled[0:train_set_size].target.tolist()}\n",
    "    test_set = {'data':shuffled[train_set_size:].data.tolist(), 'target_names':target_names, 'target':shuffled[train_set_size:].target.tolist()}\n",
    "    return train_set, test_set\n",
    "\n",
    "rtrain, rtest = get_term_doc_test_train_sets(\"/home/tessa/reading_age/projects/reddit_gen_spef/1_chunk/scored.csv\", col = 'Name', train_set_size = 4000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorize(term_doc, how = 'count'):\n",
    "    if how == 'count':\n",
    "        vectorizer = CountVectorizer(stop_words='english')\n",
    "    elif how == 'tfidf':\n",
    "        vectorizer = TfidfVectorizer(stop_words = 'english')\n",
    "    vectors = vectorizer.fit_transform(term_doc['data']).todense()\n",
    "    vocab = np.array(vectorizer.get_feature_names())\n",
    "    return vectors, vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "def topics_from_svd(vectors, vocab, num_top_words=10):\n",
    "    U, s, Vh = linalg.svd(vectors, full_matrices=False)\n",
    "    topics = show_topics(Vh, vocab, num_top_words=10)\n",
    "    return topics\n",
    "# rvectors, rvocab = vectorize(rtrain, how = 'tfidf')\n",
    "# topics_from_svd(rvectors, rvocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['like just people don really think know ve thing say good going actually way didn lot time feel drag did',\n",
       " 'http com www https org amp wiki wikipedia reddit en youtube comments watch jpg imgur html link edit article google',\n",
       " 'game dm players play amp character group new playing time want make campaign player rules fun friends set ve need',\n",
       " 'god gt believe jesus christ bible faith christian church sin life christians pray love word scripture don things lord says']"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def topics_from_nmf(vectors, vocab, num_topics, num_top_words = 10):\n",
    "    m,n=vectors.shape\n",
    "    d=num_topics\n",
    "    clf = decomposition.NMF(n_components=d, random_state=1)\n",
    "\n",
    "    W1 = clf.fit_transform(vectors)\n",
    "    H1 = clf.components_\n",
    "    topics = show_topics(H1, vocab, num_top_words = num_top_words)\n",
    "    return topics\n",
    "rvectors, rvocab = vectorize(rtrain, how = 'tfidf')\n",
    "topics_from_nmf(rvectors, rvocab, 4, num_top_words = 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['just really ve time good thing did didn actually think',\n",
       " 'com www http https reddit comments youtube watch imgur jpg',\n",
       " 'game dm players play group character new campaign rules player',\n",
       " 'god love jesus believe sin pray word life faith lord',\n",
       " 'wiki org wikipedia http en article html www english page',\n",
       " 'people don know think want need ll things way say',\n",
       " 'amp google https hl utf playing books source com new',\n",
       " 'church christian christ believe bible christians jesus faith catholic christianity',\n",
       " 'like feel drag cracker trixie looks look queens aquaria queen',\n",
       " 'gt men man gods long lust women assume understand saying']"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rvectors, rvocab = vectorize(rtrain, how = 'tfidf')\n",
    "topics_from_nmf(rvectors, rvocab, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['just really ve time good thing did didn actually think got going thought bad said way mean use say doesn',\n",
       " 'com www http https reddit comments youtube watch imgur jpg edit link html todayilearned video rupaulsdragrace autplayed ignoreme imguralbumbot thread',\n",
       " 'game dm players play group character new campaign rules player make playing want time fun friends set characters help need',\n",
       " 'god love jesus believe sin pray word life faith lord bible things man christ father holy grace doesn scripture plan',\n",
       " 'wiki org wikipedia http en article html www english page crazy jpg ones different french uk nuclear billion number cot',\n",
       " 'people don know think want need ll things way say really understand wrong make hate care drag does fact problem',\n",
       " 'amp google https hl utf playing books source com new sa played search looking imguralbumbot autplayed ignoreme biw games bih',\n",
       " 'church christian christ believe bible christians jesus faith catholic christianity years say scripture says gospel saying true good marriage yes',\n",
       " 'like feel drag cracker trixie looks look queens aquaria queen sasha stuff shit lot race love oh yeah white going',\n",
       " 'gt men man gods long lust women assume understand saying going leung þe þat case second opinion sterilized act new']"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rvectors, rvocab = vectorize(rtrain, how = 'tfidf')\n",
    "topics_from_nmf(rvectors, rvocab, 10, num_top_words = 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Name</th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Peter_Pan</td>\n",
       "      <td>PREFACE Sir J. M. Barrie's delightful creation...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Peter_Pan</td>\n",
       "      <td>Then she turned the bedclothes neatly down and...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Peter_Pan</td>\n",
       "      <td>Darling told him about the weird apparition at...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Peter_Pan</td>\n",
       "      <td>Suddenly the night-lights flickered, waned, an...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Peter_Pan</td>\n",
       "      <td>No wonder he was crying! But that was not the ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5271</th>\n",
       "      <td>Three_men_in_a_boat-Jerome_K</td>\n",
       "      <td>George took the fun more soberly, and stuck to...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5272</th>\n",
       "      <td>Three_men_in_a_boat-Jerome_K</td>\n",
       "      <td>Harris said he would introduce us both to the ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5273</th>\n",
       "      <td>Three_men_in_a_boat-Jerome_K</td>\n",
       "      <td>After which, we managed to get some fitful slu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5274</th>\n",
       "      <td>Three_men_in_a_boat-Jerome_K</td>\n",
       "      <td>“Yes it’s almost a pity we’ve made up our mind...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5275</th>\n",
       "      <td>Three_men_in_a_boat-Jerome_K</td>\n",
       "      <td>Our fine bronzed countenances and picturesque ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5276 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                              Name  \\\n",
       "0                        Peter_Pan   \n",
       "1                        Peter_Pan   \n",
       "2                        Peter_Pan   \n",
       "3                        Peter_Pan   \n",
       "4                        Peter_Pan   \n",
       "...                            ...   \n",
       "5271  Three_men_in_a_boat-Jerome_K   \n",
       "5272  Three_men_in_a_boat-Jerome_K   \n",
       "5273  Three_men_in_a_boat-Jerome_K   \n",
       "5274  Three_men_in_a_boat-Jerome_K   \n",
       "5275  Three_men_in_a_boat-Jerome_K   \n",
       "\n",
       "                                                   Text  \n",
       "0     PREFACE Sir J. M. Barrie's delightful creation...  \n",
       "1     Then she turned the bedclothes neatly down and...  \n",
       "2     Darling told him about the weird apparition at...  \n",
       "3     Suddenly the night-lights flickered, waned, an...  \n",
       "4     No wonder he was crying! But that was not the ...  \n",
       "...                                                 ...  \n",
       "5271  George took the fun more soberly, and stuck to...  \n",
       "5272  Harris said he would introduce us both to the ...  \n",
       "5273  After which, we managed to get some fitful slu...  \n",
       "5274  “Yes it’s almost a pity we’ve made up our mind...  \n",
       "5275  Our fine bronzed countenances and picturesque ...  \n",
       "\n",
       "[5276 rows x 2 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# topic modelling - try and divide some texts into topics\n",
    "# first get a dataframe of different texts\n",
    "\n",
    "novels = pd.read_csv(\"/home/tessa/reading_age/projects/24_novels/chunks/scored.csv\")\n",
    "novels = novels[[\"Name\",\"Text\"]]\n",
    "novels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Peter_Pan',\n",
       " 'Ulysses',\n",
       " 'A_Study_in_Scarlet',\n",
       " 'A_Midsummer_Nights_Dream',\n",
       " 'Alice_in_Wonderland',\n",
       " 'Dracula',\n",
       " 'Five_Famous_Fairy_Tales',\n",
       " 'Gullivers_Travels',\n",
       " 'Les_Miserables',\n",
       " 'Nicholas_Nickleby',\n",
       " 'Pride_And_Prejudice',\n",
       " 'Romeo_and_Juliet',\n",
       " 'Sleeping_Beauty',\n",
       " 'The_Disappearance_of_Lady_Frances_Carfax',\n",
       " 'The_Wind_in_the_Willows',\n",
       " 'The_Tragical_History_of_Doctor_Faustus',\n",
       " 'Three_men_in_a_boat-Jerome_K']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_names = novels.Name.unique().tolist()\n",
    "target_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Name</th>\n",
       "      <th>Text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Peter_Pan</td>\n",
       "      <td>PREFACE Sir J. M. Barrie's delightful creation...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Peter_Pan</td>\n",
       "      <td>Then she turned the bedclothes neatly down and...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Peter_Pan</td>\n",
       "      <td>Darling told him about the weird apparition at...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Peter_Pan</td>\n",
       "      <td>Suddenly the night-lights flickered, waned, an...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Peter_Pan</td>\n",
       "      <td>No wonder he was crying! But that was not the ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5271</th>\n",
       "      <td>Three_men_in_a_boat-Jerome_K</td>\n",
       "      <td>George took the fun more soberly, and stuck to...</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5272</th>\n",
       "      <td>Three_men_in_a_boat-Jerome_K</td>\n",
       "      <td>Harris said he would introduce us both to the ...</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5273</th>\n",
       "      <td>Three_men_in_a_boat-Jerome_K</td>\n",
       "      <td>After which, we managed to get some fitful slu...</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5274</th>\n",
       "      <td>Three_men_in_a_boat-Jerome_K</td>\n",
       "      <td>“Yes it’s almost a pity we’ve made up our mind...</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5275</th>\n",
       "      <td>Three_men_in_a_boat-Jerome_K</td>\n",
       "      <td>Our fine bronzed countenances and picturesque ...</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5276 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                              Name  \\\n",
       "0                        Peter_Pan   \n",
       "1                        Peter_Pan   \n",
       "2                        Peter_Pan   \n",
       "3                        Peter_Pan   \n",
       "4                        Peter_Pan   \n",
       "...                            ...   \n",
       "5271  Three_men_in_a_boat-Jerome_K   \n",
       "5272  Three_men_in_a_boat-Jerome_K   \n",
       "5273  Three_men_in_a_boat-Jerome_K   \n",
       "5274  Three_men_in_a_boat-Jerome_K   \n",
       "5275  Three_men_in_a_boat-Jerome_K   \n",
       "\n",
       "                                                   Text  target  \n",
       "0     PREFACE Sir J. M. Barrie's delightful creation...       0  \n",
       "1     Then she turned the bedclothes neatly down and...       0  \n",
       "2     Darling told him about the weird apparition at...       0  \n",
       "3     Suddenly the night-lights flickered, waned, an...       0  \n",
       "4     No wonder he was crying! But that was not the ...       0  \n",
       "...                                                 ...     ...  \n",
       "5271  George took the fun more soberly, and stuck to...      16  \n",
       "5272  Harris said he would introduce us both to the ...      16  \n",
       "5273  After which, we managed to get some fitful slu...      16  \n",
       "5274  “Yes it’s almost a pity we’ve made up our mind...      16  \n",
       "5275  Our fine bronzed countenances and picturesque ...      16  \n",
       "\n",
       "[5276 rows x 3 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "novels[\"target\"] = novels.apply(lambda x: target_names.index(x.Name), axis = 1)\n",
    "novels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "novels = novels.rename(columns = {'Text':'data'})\n",
    "novels = novels[[\"target\",\"data\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "shuffled = novels.sample(frac =1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set = {'data':shuffled[0:4000].data.tolist(), 'target_names':target_names, 'target':shuffled[0:4000].target.tolist()}\n",
    "test_set = {'data':shuffled[4000:].data.tolist(), 'target_names':target_names, 'target':shuffled[4000:].target.tolist()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>contained</th>\n",
       "      <th>Oxford</th>\n",
       "      <th>descends</th>\n",
       "      <th>place</th>\n",
       "      <th>occurred</th>\n",
       "      <th>talk</th>\n",
       "      <th>high</th>\n",
       "      <th>out</th>\n",
       "      <th>need</th>\n",
       "      <th>fallen</th>\n",
       "      <th>...</th>\n",
       "      <th>immense</th>\n",
       "      <th>obligations</th>\n",
       "      <th>be</th>\n",
       "      <th>few</th>\n",
       "      <th>below</th>\n",
       "      <th>not</th>\n",
       "      <th>you</th>\n",
       "      <th>called</th>\n",
       "      <th>respects</th>\n",
       "      <th>several</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.002795</td>\n",
       "      <td>0.002063</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.005591</td>\n",
       "      <td>0.005591</td>\n",
       "      <td>0.003095</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.002795</td>\n",
       "      <td>...</td>\n",
       "      <td>0.002795</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.002795</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.003095</td>\n",
       "      <td>0.002795</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.003231</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.003231</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.003231</td>\n",
       "      <td>0.003578</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.001193</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.006462</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.002876</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.001061</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.001061</td>\n",
       "      <td>0.002876</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.002123</td>\n",
       "      <td>0.002876</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.005752</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.002876</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows × 498 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   contained    Oxford  descends     place  occurred      talk      high  \\\n",
       "0   0.000000  0.000000  0.002795  0.002063  0.000000  0.005591  0.005591   \n",
       "1   0.000000  0.003231  0.000000  0.000000  0.003231  0.000000  0.000000   \n",
       "2   0.002876  0.000000  0.000000  0.001061  0.000000  0.000000  0.000000   \n",
       "\n",
       "        out      need    fallen  ...   immense  obligations        be  \\\n",
       "0  0.003095  0.000000  0.002795  ...  0.002795     0.000000  0.000000   \n",
       "1  0.000000  0.000000  0.000000  ...  0.000000     0.003231  0.003578   \n",
       "2  0.001061  0.002876  0.000000  ...  0.000000     0.000000  0.002123   \n",
       "\n",
       "        few     below       not       you    called  respects   several  \n",
       "0  0.000000  0.002795  0.000000  0.003095  0.002795  0.000000  0.000000  \n",
       "1  0.000000  0.000000  0.000000  0.001193  0.000000  0.000000  0.006462  \n",
       "2  0.002876  0.000000  0.005752  0.000000  0.000000  0.002876  0.000000  \n",
       "\n",
       "[3 rows x 498 columns]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_tfidf(train_set['data'][0:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "vectorizer = CountVectorizer(stop_words='english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# so this just gives a count matrix of the number of times each word appears (ignoring stop words)\n",
    "vectors = vectorizer.fit_transform(train_set['data']).todense()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = np.array(vectorizer.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 17min 38s, sys: 23min 47s, total: 41min 25s\n",
      "Wall time: 2min 51s\n"
     ]
    }
   ],
   "source": [
    "# now factorizing the matrix into 3 other matricies\n",
    "%time U, s, Vh = linalg.svd(vectors, full_matrices=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['miss squeers nicholas says man marius know did father john',\n",
       " 'miss bloom man mantalini squeers door la old nickleby madame',\n",
       " 'man jean faustus valjean good squeers know javert toad shall',\n",
       " 'marius faustus sir squeers shall cosette know come soul thou',\n",
       " 'squeers ralph romeo love mrs boys don boy little juliet',\n",
       " 'ralph mr know old mantalini newman nickleby don love arthur',\n",
       " 'know sir like bloom toad time miss did ll don',\n",
       " 'ralph old door house toad marius says room elizabeth rue',\n",
       " 'ralph little says newman mantalini miss eyes did cosette sir',\n",
       " 'sir old good miss mulberry young say lady day shall']"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# lots of references to valjean, and nicholas - seems to be overly weighted by the longest books \n",
    "def show_topics(a, vocab, num_top_words=10):\n",
    "    top_words = lambda t: [vocab[i] for i in np.argsort(t)[:-num_top_words-1:-1]]\n",
    "    topic_words = ([top_words(t) for t in a])\n",
    "    return [' '.join(t) for t in topic_words]\n",
    "\n",
    "show_topics(Vh[10:20], vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# m is equal to r in my SVD and the one in the example - why?\n",
    "tf_vectorizer = TfidfVectorizer(stop_words = 'english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_vocab = np.array(tf_vectorizer.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_vectors = tf_vectorizer.fit_transform(train_set['data']).todense()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4000, 41889)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf_vectors.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.46616561253062055"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.max(tf_vectors[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 18min, sys: 24min 52s, total: 42min 53s\n",
      "Wall time: 3min 51s\n"
     ]
    }
   ],
   "source": [
    "%time tU, ts, tVh = linalg.svd(tf_vectors, full_matrices=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4000, 4000) (4000,) (4000, 41889)\n"
     ]
    }
   ],
   "source": [
    "print(tU.shape, ts.shape, tVh.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['autem irving leopoldi kossuth eius breastplate legplates tyler generatio vocabitur',\n",
       " 'nicholas mr squeers mrs miss nickleby elizabeth ralph kate darcy',\n",
       " 'elizabeth darcy jane bennet bingley mr collins wickham lydia catherine',\n",
       " 'jean valjean cosette marius nicholas squeers javert nickleby mr miss',\n",
       " 'toad rat mole van helsing alice lucy mina went badger',\n",
       " 'ralph nickleby kate mantalini mulberry newman arthur mrs wititterly helsing',\n",
       " 'squeers helsing van lucy mina jonathan arthur dracula valjean professor',\n",
       " 'valjean jean toad rat mole javert badger alice ll cosette',\n",
       " 'nicholas crummles newman tim smike linkinwater manager brother creevy la',\n",
       " 'marius cosette toad rat mole nicholas badger thénardier know gillenormand']"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_topics(tVh[0:10], tf_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['said know don alice asked ll went like yes come',\n",
       " 'man little did like old time marius jean valjean day',\n",
       " 'mr elizabeth mrs darcy bennet miss jane bingley lady collins',\n",
       " 'thou romeo love thy thee faustus shall juliet come ll',\n",
       " 'nicholas squeers miss nickleby sir mrs said ralph kate replied']"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# this looks way better than the svd -> i've only done this with 5 topics but would be interesting to expand\n",
    "m,n=vectors.shape\n",
    "d=5\n",
    "clf = decomposition.NMF(n_components=d, random_state=1)\n",
    "\n",
    "W1 = clf.fit_transform(vectors)\n",
    "H1 = clf.components_\n",
    "show_topics(H1, vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['said know don alice asked ll went like yes come',\n",
       " 'man little did like old time marius jean valjean day',\n",
       " 'mr elizabeth mrs darcy bennet miss jane bingley lady collins',\n",
       " 'thou romeo love thy thee faustus shall juliet come ll',\n",
       " 'nicholas squeers miss nickleby sir mrs said ralph kate replied']"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_topics(H1, vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['said little man like old alice time did bloom great',\n",
       " 'valjean jean cosette javert fauchelevent madeleine rue man convict did',\n",
       " 'elizabeth mr darcy bennet jane bingley mrs miss collins wickham',\n",
       " 'ralph nickleby kate mrs said mantalini sir mulberry mr miss',\n",
       " 'romeo thou juliet thy thee faustus love capulet friar nurse',\n",
       " 'squeers miss boys mr said nicholas boy replied tilda smike',\n",
       " 'helsing van lucy mina jonathan arthur shall professor dracula count',\n",
       " 'toad rat mole badger ll said ve river boat animal',\n",
       " 'nicholas newman crummles said mr smike tim noggs replied sir',\n",
       " 'marius cosette thénardier father gillenormand man courfeyrac rue jondrette francs']"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tm,tn=vectors.shape\n",
    "td=10\n",
    "clf = decomposition.NMF(n_components=td, random_state=1)\n",
    "\n",
    "tW1 = clf.fit_transform(tf_vectors)\n",
    "tH1 = clf.components_\n",
    "show_topics(tH1, tf_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
